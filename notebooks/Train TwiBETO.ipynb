{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4802 2443 7264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Index(['text', 'polarity', 'label'], dtype='object'),\n",
       " Index(['text', 'polarity', 'label'], dtype='object'),\n",
       " Index(['text', 'polarity', 'label'], dtype='object'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def get_lang(file):\n",
    "    return os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "\"\"\"\n",
    "Lo pongo as√≠ por hugginface\n",
    "\"\"\"\n",
    "id2label = {0: 'N', 1: 'NEU', 2: 'P'}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "def load_df(file):\n",
    "    dialect = get_lang(file)\n",
    "    \n",
    "    df = pd.read_table(file, names=[\"id\", \"text\", \"polarity\"], index_col=0)\n",
    "    #df[\"dialect\"] = dialect\n",
    "    \n",
    "    for label, idx in label2id.items():\n",
    "        df.loc[df[\"polarity\"] == label, \"label\"] = idx\n",
    "    return df\n",
    "\n",
    "train_files = glob(\"../data/tass2020/train/*.tsv\")\n",
    "dev_files = glob(\"../data/tass2020/dev/*.tsv\")\n",
    "test_files = glob(\"../data/tass2020/test1.1/*.tsv\")\n",
    "\n",
    "train_dfs = {get_lang(file):load_df(file) for file in train_files}\n",
    "dev_dfs = {get_lang(file):load_df(file) for file in dev_files}\n",
    "test_dfs = {get_lang(file):load_df(file) for file in test_files}\n",
    "\n",
    "train_df = pd.concat(train_dfs.values())\n",
    "dev_df = pd.concat(dev_dfs.values())\n",
    "test_df = pd.concat(test_dfs.values())\n",
    "\n",
    "print(len(train_df), len(dev_df), len(test_df))\n",
    "\n",
    "train_df.columns, dev_df.columns, test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_df[\"label\"].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Convertimos todos los usuarios al string \"usuario\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4802, 3), (2443, 3))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, dev_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_tweet)\n",
    "dev_df[\"text\"] = dev_df[\"text\"].apply(preprocess_tweet)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./TwiBETO were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./TwiBETO and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model_name = './TwiBETO'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "tokenizer.model_max_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'N', 1: 'NEU', 2: 'P'}, {'N': 0, 'NEU': 1, 'P': 2})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label, model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos primero las longitudes (a ver si no hay nada mal cargado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "\n",
    "examples = pd.concat([train_df, dev_df])\n",
    "\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'label': ClassLabel(num_classes=3, names=[\"neg\", \"neu\", \"pos\"])\n",
    "})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label\"]], features=features)\n",
    "dev_dataset = Dataset.from_pandas(dev_df[[\"text\", \"label\"]], features=features)\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\", \"label\"]], features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=3, names=['neg', 'neu', 'pos'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b83abed701492f998a0db29d7cf719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970b2d7a63a8436daa9f122fde45724f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=153.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59922df6fe2b42db9e9250253eddc103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=454.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "dev_dataset = dev_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecaa1ff697a440c8b58dcbe7c76cd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4802.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8219a80bc2ee446abef4e5b22e620cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2443.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1469c2df91f4aa4a74c70f168605a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7264.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(dataset):\n",
    "    dataset = dataset.map(lambda examples: {'labels': examples['label']})\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "    return dataset\n",
    "\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "dev_dataset = format_dataset(dev_dataset)\n",
    "test_dataset = format_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./TwiBETO were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./TwiBETO and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext import data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_it = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "dev_it = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size)\n",
    "test_it = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=3)\n",
    "\n",
    "model.config.hidden_dropout_prob = 0.20\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "model = model.to(device)\n",
    "model.train();\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, it):\n",
    "    \"\"\"\n",
    "    Calculates labels and predictions for it\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    (labels, preds): torch.Tensor\n",
    "    \n",
    "    where labels are the true labels\n",
    "    and preds are the predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(it)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            losses.append(loss.item())\n",
    "            outs = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "            true.append(batch['labels'].cpu())\n",
    "            preds.append(outs.cpu())\n",
    "    return np.array(losses).mean(), torch.cat(true), torch.cat(preds).argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08abcb0a5c8644d199a49516419cb012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed21a752485c49648abf3de55fe16cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 \n",
      "Train loss 0.8995\n",
      "Dev loss 0.8002\n",
      "Dev metrics {'accuracy': 0.6553417928776095, 'f1': 0.6343417404729942, 'precision': 0.6547883823566102, 'recall': 0.6457086514276349}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e1299b4f7c4ea8b5530b3773c6141d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf7a0987e5e4020bce008a157bdd968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 \n",
      "Train loss 0.5990\n",
      "Dev loss 0.7975\n",
      "Dev metrics {'accuracy': 0.6713057715923045, 'f1': 0.6594151953577297, 'precision': 0.6625478999401578, 'recall': 0.6654176046350669}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c1510efa6941a1a20477c910eff465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc24fe5cd16241acb73c4aa9a034054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 \n",
      "Train loss 0.4177\n",
      "Dev loss 0.8654\n",
      "Dev metrics {'accuracy': 0.6639377814162915, 'f1': 0.6490162898628277, 'precision': 0.6543944670721568, 'recall': 0.6564646914289053}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4ad4b4f00d4f2381f1851f55cfdff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ff291514e04ab18cb727d870843869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 \n",
      "Train loss 0.2967\n",
      "Dev loss 0.8983\n",
      "Dev metrics {'accuracy': 0.6672124437167417, 'f1': 0.661895283338922, 'precision': 0.6597058834539672, 'recall': 0.6657435067836439}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb3b67abced487fab07c2b3ca215f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a874949306a74972b3151a579f0a09ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 \n",
      "Train loss 0.2177\n",
      "Dev loss 0.9644\n",
      "Dev metrics {'accuracy': 0.66189111747851, 'f1': 0.6666989647137118, 'precision': 0.681483306404647, 'recall': 0.6634852271095005}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.train().to(device)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "num_training_steps = epochs * len(train_it)\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    for i, batch in enumerate(tqdm(train_it)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_loss = np.array(train_losses).mean()\n",
    "    dev_loss, dev_labels, dev_preds = evaluate(model, dev_it)\n",
    "    dev_metrics = compute_metrics(dev_labels, dev_preds) \n",
    "    \n",
    "    print(f\"Epoch {epoch:<2}\")\n",
    "    print(f\"Train loss {train_loss:.4f}\")\n",
    "    print(f\"Dev loss {dev_loss:.4f}\")\n",
    "    print(f\"Dev metrics {dev_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 0.673438568716703\n",
    "2. 0.6734036976999446\n",
    "3. 0.6657802770738191\n",
    "4. 0.6637223187531293\n",
    "5. 0.6666989647137118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6686), tensor(0.0045))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t = torch.Tensor([\n",
    "    0.673438568716703,\n",
    "    0.6734036976999446,\n",
    "    0.6657802770738191,\n",
    "    0.6637223187531293,\n",
    "    0.6666989647137118,\n",
    "])\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label[0] = \"NEG\"\n",
    "model.config.id2label[2] = \"POS\"\n",
    "\n",
    "model.config.label2id = {v:k for k,v in model.config.id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/beto-sentiment-analysis/vocab.txt',\n",
       " '../models/beto-sentiment-analysis/special_tokens_map.json',\n",
       " '../models/beto-sentiment-analysis/added_tokens.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../models/beto-sentiment-analysis\"\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
