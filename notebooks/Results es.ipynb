{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In this notebook we show the results of our experiments for both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "bertin.json\t   birnn_cc.json       mbert_uncased.json  robertuito.json\n",
      "bertweet.json\t   birnn_twitter.json  rnn_cc.json\n",
      "beto_cased.json    ffn_cc.json\t       rnn_twitter.json\n",
      "beto_uncased.json  ffn_twitter.json    roberta.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "base_path = \"../evaluations/es\"\n",
    "!ls $base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../evaluations/es/birnn_twitter.json\n",
      "../evaluations/es/ffn_twitter.json\n",
      "../evaluations/es/roberta.json\n",
      "../evaluations/es/beto_uncased.json\n",
      "../evaluations/es/mbert_uncased.json\n",
      "../evaluations/es/birnn_cc.json\n",
      "../evaluations/es/bertweet.json\n",
      "../evaluations/es/robertuito.json\n",
      "../evaluations/es/bertin.json\n",
      "../evaluations/es/rnn_twitter.json\n",
      "../evaluations/es/beto_cased.json\n",
      "../evaluations/es/rnn_cc.json\n",
      "../evaluations/es/ffn_cc.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['birnn_twitter', 'ffn_twitter', 'roberta', 'beto_uncased', 'mbert_uncased', 'birnn_cc', 'bertweet', 'robertuito', 'bertin', 'rnn_twitter', 'beto_cased', 'rnn_cc', 'ffn_cc'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def clean_key(k):\n",
    "    return k.split(\"_\", 1)[1]\n",
    "\n",
    "\n",
    "evaluation_paths = glob.glob(f\"{base_path}/*.json\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "for path in evaluation_paths:\n",
    "    print(path)\n",
    "    name = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path) as f:\n",
    "        model_evaluation = json.load(f)\n",
    "        clean_evaluations = []\n",
    "        for task in model_evaluation[\"evaluations\"].keys():\n",
    "            task_evaluations = model_evaluation[\"evaluations\"][task]\n",
    "            clean_evaluations = [\n",
    "                {clean_key(k): v for k, v in ev.items()} \n",
    "                for ev in task_evaluations\n",
    "            ]\n",
    "\n",
    "            model_evaluation[\"evaluations\"][task] = clean_evaluations\n",
    "        models[name] = model_evaluation\n",
    "        \n",
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "birnn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "ffn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "roberta\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "beto_uncased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "mbert_uncased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "birnn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "bertweet\n",
      "hate_speech\n",
      "1\n",
      "sentiment\n",
      "1\n",
      "emotion\n",
      "1\n",
      "irony\n",
      "0\n",
      "==================================================\n",
      "robertuito\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "bertin\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "9\n",
      "==================================================\n",
      "rnn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "beto_cased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "rnn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "ffn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"hate_speech\": \"macro_f1\",\n",
    "    \"sentiment\": \"macro_f1\",\n",
    "    \"emotion\": \"macro_f1\",\n",
    "    \"irony\": \"macro_f1\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, model_evaluation in models.items():\n",
    "    print(\"=\"*50)\n",
    "    print(model)\n",
    "    for task, task_evaluations in model_evaluation[\"evaluations\"].items():\n",
    "        print(task)\n",
    "        print(len(task_evaluations))\n",
    "        for evaluation in task_evaluations:\n",
    "            metric = metrics[task]\n",
    "\n",
    "            ## TODO\n",
    "            if metric not in evaluation:\n",
    "                metric = \"hateful_f1\"\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"task\": task,\n",
    "                \"metric\": evaluation[metric],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>emotion</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>irony</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>robertuito</th>\n",
       "      <td>0.560</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.527</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertin</th>\n",
       "      <td>0.524</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_uncased</th>\n",
       "      <td>0.532</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_cased</th>\n",
       "      <td>0.516</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_uncased</th>\n",
       "      <td>0.493</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_twitter</th>\n",
       "      <td>0.264</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_twitter</th>\n",
       "      <td>0.269</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_cc</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_cc</th>\n",
       "      <td>0.237</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_twitter</th>\n",
       "      <td>0.202</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_cc</th>\n",
       "      <td>0.173</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertweet</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task           emotion  hate_speech  irony  sentiment  score\n",
       "model                                                       \n",
       "robertuito       0.560        0.759  0.739      0.705  0.691\n",
       "roberta          0.527        0.741  0.721      0.670  0.665\n",
       "bertin           0.524        0.738  0.713      0.666  0.660\n",
       "beto_uncased     0.532        0.727  0.701      0.651  0.653\n",
       "beto_cased       0.516        0.724  0.705      0.662  0.652\n",
       "mbert_uncased    0.493        0.718  0.681      0.617  0.627\n",
       "birnn_twitter    0.264        0.592  0.631      0.585  0.518\n",
       "rnn_twitter      0.269        0.538  0.628      0.602  0.509\n",
       "birnn_cc         0.231        0.534  0.625      0.553  0.486\n",
       "rnn_cc           0.237        0.516  0.581      0.564  0.474\n",
       "ffn_twitter      0.202        0.444  0.579      0.538  0.441\n",
       "ffn_cc           0.173        0.392  0.511      0.511  0.397\n",
       "bertweet         0.094        0.000    nan      0.192  0.095"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "mean_df = pd.DataFrame(results).groupby([\"model\", \"task\"]).mean().stack()\n",
    "std_df = pd.DataFrame(results).groupby([\"model\", \"task\"]).std().stack()\n",
    "# Magia negra\n",
    "mean_df.index = mean_df.index.droplevel(-1)\n",
    "std_df.index = std_df.index.droplevel(-1)\n",
    "\n",
    "mean_df = mean_df.unstack(1)\n",
    "std_df = std_df.unstack(1)\n",
    "\n",
    "mean_df[\"score\"] = mean_df.mean(1)\n",
    "\n",
    "mean_df.sort_values(\"score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>irony</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>robertuito</th>\n",
       "      <td>0.560 ± 0.010</td>\n",
       "      <td>0.759 ± 0.007</td>\n",
       "      <td>0.739 ± 0.005</td>\n",
       "      <td>0.705 ± 0.003</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.527 ± 0.015</td>\n",
       "      <td>0.741 ± 0.012</td>\n",
       "      <td>0.721 ± 0.008</td>\n",
       "      <td>0.670 ± 0.006</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertin</th>\n",
       "      <td>0.524 ± 0.007</td>\n",
       "      <td>0.738 ± 0.007</td>\n",
       "      <td>0.713 ± 0.012</td>\n",
       "      <td>0.666 ± 0.005</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_uncased</th>\n",
       "      <td>0.532 ± 0.012</td>\n",
       "      <td>0.727 ± 0.016</td>\n",
       "      <td>0.701 ± 0.007</td>\n",
       "      <td>0.651 ± 0.006</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_cased</th>\n",
       "      <td>0.516 ± 0.012</td>\n",
       "      <td>0.724 ± 0.012</td>\n",
       "      <td>0.705 ± 0.009</td>\n",
       "      <td>0.662 ± 0.005</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_uncased</th>\n",
       "      <td>0.493 ± 0.010</td>\n",
       "      <td>0.718 ± 0.011</td>\n",
       "      <td>0.681 ± 0.010</td>\n",
       "      <td>0.617 ± 0.003</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_twitter</th>\n",
       "      <td>0.264 ± 0.007</td>\n",
       "      <td>0.592 ± 0.018</td>\n",
       "      <td>0.631 ± 0.011</td>\n",
       "      <td>0.585 ± 0.011</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_twitter</th>\n",
       "      <td>0.269 ± 0.003</td>\n",
       "      <td>0.538 ± 0.014</td>\n",
       "      <td>0.628 ± 0.014</td>\n",
       "      <td>0.602 ± 0.004</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_cc</th>\n",
       "      <td>0.231 ± 0.006</td>\n",
       "      <td>0.534 ± 0.022</td>\n",
       "      <td>0.625 ± 0.009</td>\n",
       "      <td>0.553 ± 0.008</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_cc</th>\n",
       "      <td>0.237 ± 0.005</td>\n",
       "      <td>0.516 ± 0.006</td>\n",
       "      <td>0.581 ± 0.016</td>\n",
       "      <td>0.564 ± 0.004</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_twitter</th>\n",
       "      <td>0.202 ± 0.002</td>\n",
       "      <td>0.444 ± 0.005</td>\n",
       "      <td>0.579 ± 0.012</td>\n",
       "      <td>0.538 ± 0.005</td>\n",
       "      <td>0.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_cc</th>\n",
       "      <td>0.173 ± 0.002</td>\n",
       "      <td>0.392 ± 0.001</td>\n",
       "      <td>0.511 ± 0.005</td>\n",
       "      <td>0.511 ± 0.003</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     emotion    hate_speech          irony      sentiment  \\\n",
       "model                                                                       \n",
       "robertuito     0.560 ± 0.010  0.759 ± 0.007  0.739 ± 0.005  0.705 ± 0.003   \n",
       "roberta        0.527 ± 0.015  0.741 ± 0.012  0.721 ± 0.008  0.670 ± 0.006   \n",
       "bertin         0.524 ± 0.007  0.738 ± 0.007  0.713 ± 0.012  0.666 ± 0.005   \n",
       "beto_uncased   0.532 ± 0.012  0.727 ± 0.016  0.701 ± 0.007  0.651 ± 0.006   \n",
       "beto_cased     0.516 ± 0.012  0.724 ± 0.012  0.705 ± 0.009  0.662 ± 0.005   \n",
       "mbert_uncased  0.493 ± 0.010  0.718 ± 0.011  0.681 ± 0.010  0.617 ± 0.003   \n",
       "birnn_twitter  0.264 ± 0.007  0.592 ± 0.018  0.631 ± 0.011  0.585 ± 0.011   \n",
       "rnn_twitter    0.269 ± 0.003  0.538 ± 0.014  0.628 ± 0.014  0.602 ± 0.004   \n",
       "birnn_cc       0.231 ± 0.006  0.534 ± 0.022  0.625 ± 0.009  0.553 ± 0.008   \n",
       "rnn_cc         0.237 ± 0.005  0.516 ± 0.006  0.581 ± 0.016  0.564 ± 0.004   \n",
       "ffn_twitter    0.202 ± 0.002  0.444 ± 0.005  0.579 ± 0.012  0.538 ± 0.005   \n",
       "ffn_cc         0.173 ± 0.002  0.392 ± 0.001  0.511 ± 0.005  0.511 ± 0.003   \n",
       "\n",
       "               score  \n",
       "model                 \n",
       "robertuito     0.691  \n",
       "roberta        0.665  \n",
       "bertin         0.660  \n",
       "beto_uncased   0.653  \n",
       "beto_cased     0.652  \n",
       "mbert_uncased  0.627  \n",
       "birnn_twitter  0.518  \n",
       "rnn_twitter    0.509  \n",
       "birnn_cc       0.486  \n",
       "rnn_cc         0.474  \n",
       "ffn_twitter    0.441  \n",
       "ffn_cc         0.397  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for col in mean_df:\n",
    "    if col == \"score\":\n",
    "        continue\n",
    "    result_df[col] = mean_df[col].apply(lambda x: f\"{x:.3f}\") + \" ± \" + std_df[col].apply(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "result_df[\"score\"] = mean_df[\"score\"]\n",
    "\n",
    "result_df = result_df.sort_values(\"score\", ascending=False)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model         | emotion       | hate_speech   | irony         | sentiment     |    score |\n",
      "|:--------------|:--------------|:--------------|:--------------|:--------------|---------:|\n",
      "| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 | 0.690734 |\n",
      "| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 | 0.664632 |\n",
      "| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 | 0.660141 |\n",
      "| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 | 0.652608 |\n",
      "| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705 ± 0.009 | 0.662 ± 0.005 | 0.651739 |\n",
      "| mbert_uncased | 0.493 ± 0.010 | 0.718 ± 0.011 | 0.681 ± 0.010 | 0.617 ± 0.003 | 0.627368 |\n",
      "| birnn_twitter | 0.264 ± 0.007 | 0.592 ± 0.018 | 0.631 ± 0.011 | 0.585 ± 0.011 | 0.518025 |\n",
      "| rnn_twitter   | 0.269 ± 0.003 | 0.538 ± 0.014 | 0.628 ± 0.014 | 0.602 ± 0.004 | 0.509427 |\n",
      "| birnn_cc      | 0.231 ± 0.006 | 0.534 ± 0.022 | 0.625 ± 0.009 | 0.553 ± 0.008 | 0.485782 |\n",
      "| rnn_cc        | 0.237 ± 0.005 | 0.516 ± 0.006 | 0.581 ± 0.016 | 0.564 ± 0.004 | 0.474446 |\n",
      "| ffn_twitter   | 0.202 ± 0.002 | 0.444 ± 0.005 | 0.579 ± 0.012 | 0.538 ± 0.005 | 0.440567 |\n",
      "| ffn_cc        | 0.173 ± 0.002 | 0.392 ± 0.001 | 0.511 ± 0.005 | 0.511 ± 0.003 | 0.396715 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(result_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "pairs = [\n",
    "    ('birnn_twitter', 'birnn_cc'),\n",
    "    ('rnn_twitter', 'rnn_cc'),\n",
    "    ('ffn_twitter', 'ffn_cc'),\n",
    "]\n",
    "\n",
    "tasks = [\"irony\", \"emotion\", \"sentiment\"]\n",
    "\n",
    "pvals = []\n",
    "\n",
    "for twitter_model, cc_model in pairs:\n",
    "    for task in tasks:\n",
    "        tw_scores = df.loc[(df[\"model\"] == twitter_model) & (df[\"task\"] == task), \"metric\"]\n",
    "        cc_scores = df.loc[(df[\"model\"] == cc_model) & (df[\"task\"] == task), \"metric\"]\n",
    "\n",
    "        pvals.append({\n",
    "            \"model\": twitter_model,\n",
    "            \"task\": task,\n",
    "            \"pval\": scipy.stats.mannwhitneyu(tw_scores, cc_scores, alternative=\"greater\").pvalue,\n",
    "        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birnn_twitter</td>\n",
       "      <td>irony</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birnn_twitter</td>\n",
       "      <td>emotion</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>birnn_twitter</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rnn_twitter</td>\n",
       "      <td>irony</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rnn_twitter</td>\n",
       "      <td>emotion</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rnn_twitter</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ffn_twitter</td>\n",
       "      <td>irony</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ffn_twitter</td>\n",
       "      <td>emotion</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ffn_twitter</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model       task  pval\n",
       "0  birnn_twitter      irony 0.038\n",
       "1  birnn_twitter    emotion 0.000\n",
       "2  birnn_twitter  sentiment 0.000\n",
       "3    rnn_twitter      irony 0.000\n",
       "4    rnn_twitter    emotion 0.000\n",
       "5    rnn_twitter  sentiment 0.000\n",
       "6    ffn_twitter      irony 0.000\n",
       "7    ffn_twitter    emotion 0.000\n",
       "8    ffn_twitter  sentiment 0.001"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "df_pvals = pd.DataFrame(pvals)\n",
    "df_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03783079, 0.00011743, 0.00011743, 0.00011743, 0.00011743,\n",
       "       0.00011743, 0.00011743, 0.00011743, 0.00073966])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multipletests(df_pvals.pval, alpha=0.05, method=\"fdr_bh\")[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pysent-oyXQVI9B': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
