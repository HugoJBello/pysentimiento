{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../evaluations/lince/ner/bert-base-cased.json\n",
      "../../evaluations/lince/ner/xlm-roberta-base.json\n",
      "../../evaluations/lince/ner/bert-base-multilingual-uncased.json\n",
      "../../evaluations/lince/ner/bert-base-spanish-wwm-uncased.json\n",
      "../../evaluations/lince/ner/robertuito-base-uncased.json\n",
      "../../evaluations/lince/ner/robertuito-base-deacc.json\n",
      "../../evaluations/lince/ner/robertuito-base-cased.json\n",
      "../../evaluations/lince/ner/bert-base-uncased.json\n",
      "../../evaluations/lince/ner/robertuito-base-uncased_nosublabeling.json\n",
      "../../evaluations/lince/ner/bert-base-spanish-wwm-cased.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def clean_key(k):\n",
    "    return k.split(\"_\", 1)[1]\n",
    "\n",
    "base_path = \"../../evaluations/lince/ner/\"\n",
    "\n",
    "evaluation_paths = glob.glob(f\"{base_path}/*.json\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "for path in evaluation_paths:\n",
    "    print(path)\n",
    "    name = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path) as f:\n",
    "        model_evaluation = json.load(f)\n",
    "        clean_evaluations = []\n",
    "        for task in model_evaluation[\"evaluations\"].keys():\n",
    "            task_evaluations = model_evaluation[\"evaluations\"][task]\n",
    "            clean_evaluations = [\n",
    "                {clean_key(k): v for k, v in ev.items()} \n",
    "                for ev in task_evaluations\n",
    "            ]\n",
    "\n",
    "            model_evaluation[\"evaluations\"][task] = clean_evaluations\n",
    "        models[name] = model_evaluation\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "bert-base-cased\n",
      "ner\n",
      "5\n",
      "==================================================\n",
      "xlm-roberta-base\n",
      "ner\n",
      "2\n",
      "==================================================\n",
      "bert-base-multilingual-uncased\n",
      "ner\n",
      "2\n",
      "==================================================\n",
      "bert-base-spanish-wwm-uncased\n",
      "ner\n",
      "4\n",
      "==================================================\n",
      "robertuito-base-uncased\n",
      "ner\n",
      "6\n",
      "==================================================\n",
      "robertuito-base-deacc\n",
      "ner\n",
      "5\n",
      "==================================================\n",
      "robertuito-base-cased\n",
      "ner\n",
      "7\n",
      "==================================================\n",
      "bert-base-uncased\n",
      "ner\n",
      "5\n",
      "==================================================\n",
      "robertuito-base-uncased_nosublabeling\n",
      "ner\n",
      "1\n",
      "==================================================\n",
      "bert-base-spanish-wwm-cased\n",
      "ner\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for model, model_evaluation in models.items():\n",
    "    print(\"=\"*50)\n",
    "    print(model)\n",
    "    for task, task_evaluations in model_evaluation[\"evaluations\"].items():\n",
    "        print(task)\n",
    "        print(len(task_evaluations))\n",
    "        for evaluation in task_evaluations:\n",
    "\n",
    "            results.append({\n",
    "                **{\n",
    "                    \"model\": model,\n",
    "                    \"task\": task,\n",
    "                },\n",
    "                **evaluation,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>robertuito-base-uncased_nosublabeling</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertuito-base-uncased</th>\n",
       "      <td>0.649</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertuito-base-deacc</th>\n",
       "      <td>0.647</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertuito-base-cased</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.624</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-roberta-base</th>\n",
       "      <td>0.614</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-spanish-wwm-uncased</th>\n",
       "      <td>0.605</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased</th>\n",
       "      <td>0.587</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-spanish-wwm-cased</th>\n",
       "      <td>0.566</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-cased</th>\n",
       "      <td>0.554</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       micro_f1  macro_f1  accuracy  \\\n",
       "model                                                                 \n",
       "robertuito-base-uncased_nosublabeling     0.667     0.508     0.986   \n",
       "robertuito-base-uncased                   0.649     0.491     0.982   \n",
       "robertuito-base-deacc                     0.647     0.494     0.982   \n",
       "robertuito-base-cased                     0.630     0.470     0.980   \n",
       "bert-base-multilingual-uncased            0.624     0.481     0.983   \n",
       "xlm-roberta-base                          0.614     0.467     0.981   \n",
       "bert-base-spanish-wwm-uncased             0.605     0.456     0.981   \n",
       "bert-base-uncased                         0.587     0.436     0.983   \n",
       "bert-base-spanish-wwm-cased               0.566     0.414     0.977   \n",
       "bert-base-cased                           0.554     0.406     0.979   \n",
       "\n",
       "                                       precision  recall  \n",
       "model                                                     \n",
       "robertuito-base-uncased_nosublabeling      0.668   0.667  \n",
       "robertuito-base-uncased                    0.637   0.662  \n",
       "robertuito-base-deacc                      0.628   0.667  \n",
       "robertuito-base-cased                      0.608   0.654  \n",
       "bert-base-multilingual-uncased             0.604   0.646  \n",
       "xlm-roberta-base                           0.601   0.627  \n",
       "bert-base-spanish-wwm-uncased              0.597   0.613  \n",
       "bert-base-uncased                          0.578   0.596  \n",
       "bert-base-spanish-wwm-cased                0.557   0.576  \n",
       "bert-base-cased                            0.544   0.564  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "mean_df = pd.DataFrame(results).groupby([\"model\"]).mean()\n",
    "std_df = pd.DataFrame(results).groupby([\"model\"]).std()\n",
    "# Magia negra\n",
    "\n",
    "#mean_df = mean_df.unstack(1)\n",
    "#std_df = std_df.unstack(1)\n",
    "\n",
    "cols = [\"micro_f1\", \"macro_f1\", \"accuracy\", \"precision\", \"recall\"]\n",
    "\n",
    "idx = mean_df.index\n",
    "\n",
    "#mean_df.loc[idx, cols].round(3).astype(str) + \" ± \" + std_df.loc[idx, cols].round(3).astype(str)\n",
    "\n",
    "mean_df.loc[idx, cols].sort_values(by=[\"micro_f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('pysentimiento-bwlKzHxB-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
