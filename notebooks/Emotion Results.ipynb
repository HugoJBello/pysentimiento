{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../evaluations/emotion_bertweet_base.json\n",
      "../evaluations/emotion_beto.json\n",
      "../evaluations/emotion_mbert_es.json\n",
      "../evaluations/emotion_mbert_en.json\n",
      "../evaluations/emotion_roberta.json\n",
      "../evaluations/emotion_bert_base.json\n",
      "../evaluations/emotion_distilbert_en.json\n",
      "../evaluations/emotion_distilbert_es.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob \n",
    "import json \n",
    "\n",
    "evaluation_files = glob.glob(\"../evaluations/emotion_*.json\")\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for file in evaluation_files:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        evaluation = json.load(f)\n",
    "        evaluation[\"file\"] = file.split(\"/\")[-1]\n",
    "        evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Lang</th>\n      <th>Loss</th>\n      <th>Others f1</th>\n      <th>Others precision</th>\n      <th>Others recall</th>\n      <th>Joy f1</th>\n      <th>Joy precision</th>\n      <th>Joy recall</th>\n      <th>Sadness f1</th>\n      <th>...</th>\n      <th>Disgust recall</th>\n      <th>Fear f1</th>\n      <th>Fear precision</th>\n      <th>Fear recall</th>\n      <th>Macro f1</th>\n      <th>Macro precision</th>\n      <th>Macro recall</th>\n      <th>Acc</th>\n      <th>Runtime</th>\n      <th>Samples per second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>distilbert-en</td>\n      <td>en</td>\n      <td>1.378156</td>\n      <td>0.509407</td>\n      <td>0.633094</td>\n      <td>0.426150</td>\n      <td>0.665484</td>\n      <td>0.607780</td>\n      <td>0.735294</td>\n      <td>0.502165</td>\n      <td>...</td>\n      <td>0.413613</td>\n      <td>0.201835</td>\n      <td>0.154930</td>\n      <td>0.289474</td>\n      <td>0.382939</td>\n      <td>0.366219</td>\n      <td>0.418844</td>\n      <td>0.503286</td>\n      <td>5.4081</td>\n      <td>337.642</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mbert-en</td>\n      <td>en</td>\n      <td>1.386493</td>\n      <td>0.547718</td>\n      <td>0.638710</td>\n      <td>0.479419</td>\n      <td>0.650691</td>\n      <td>0.613913</td>\n      <td>0.692157</td>\n      <td>0.544643</td>\n      <td>...</td>\n      <td>0.434555</td>\n      <td>0.255319</td>\n      <td>0.214286</td>\n      <td>0.315789</td>\n      <td>0.393619</td>\n      <td>0.379647</td>\n      <td>0.419032</td>\n      <td>0.515882</td>\n      <td>9.2533</td>\n      <td>197.335</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bert-base</td>\n      <td>en</td>\n      <td>1.274540</td>\n      <td>0.593980</td>\n      <td>0.663677</td>\n      <td>0.537530</td>\n      <td>0.683521</td>\n      <td>0.654122</td>\n      <td>0.715686</td>\n      <td>0.541485</td>\n      <td>...</td>\n      <td>0.434555</td>\n      <td>0.252632</td>\n      <td>0.210526</td>\n      <td>0.315789</td>\n      <td>0.439238</td>\n      <td>0.420341</td>\n      <td>0.468907</td>\n      <td>0.559146</td>\n      <td>9.6085</td>\n      <td>190.041</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>roberta-base</td>\n      <td>en</td>\n      <td>1.233089</td>\n      <td>0.573888</td>\n      <td>0.704225</td>\n      <td>0.484262</td>\n      <td>0.689527</td>\n      <td>0.653779</td>\n      <td>0.729412</td>\n      <td>0.547945</td>\n      <td>...</td>\n      <td>0.633508</td>\n      <td>0.255814</td>\n      <td>0.229167</td>\n      <td>0.289474</td>\n      <td>0.445122</td>\n      <td>0.431733</td>\n      <td>0.490829</td>\n      <td>0.562979</td>\n      <td>9.5375</td>\n      <td>191.455</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>bertweet-base</td>\n      <td>en</td>\n      <td>1.172043</td>\n      <td>0.606019</td>\n      <td>0.696541</td>\n      <td>0.536320</td>\n      <td>0.711069</td>\n      <td>0.681655</td>\n      <td>0.743137</td>\n      <td>0.608696</td>\n      <td>...</td>\n      <td>0.523560</td>\n      <td>0.260000</td>\n      <td>0.209677</td>\n      <td>0.342105</td>\n      <td>0.475686</td>\n      <td>0.456330</td>\n      <td>0.513214</td>\n      <td>0.584337</td>\n      <td>9.2916</td>\n      <td>196.522</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>distilbert-es</td>\n      <td>es</td>\n      <td>1.118434</td>\n      <td>0.678962</td>\n      <td>0.780220</td>\n      <td>0.600967</td>\n      <td>0.602230</td>\n      <td>0.546067</td>\n      <td>0.671271</td>\n      <td>0.716707</td>\n      <td>...</td>\n      <td>0.090909</td>\n      <td>0.461538</td>\n      <td>0.352941</td>\n      <td>0.666667</td>\n      <td>0.463233</td>\n      <td>0.437966</td>\n      <td>0.512605</td>\n      <td>0.600477</td>\n      <td>7.9120</td>\n      <td>211.957</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mbert-es</td>\n      <td>es</td>\n      <td>1.136714</td>\n      <td>0.691127</td>\n      <td>0.770833</td>\n      <td>0.626360</td>\n      <td>0.585242</td>\n      <td>0.542453</td>\n      <td>0.635359</td>\n      <td>0.710462</td>\n      <td>...</td>\n      <td>0.151515</td>\n      <td>0.431373</td>\n      <td>0.333333</td>\n      <td>0.611111</td>\n      <td>0.473628</td>\n      <td>0.449092</td>\n      <td>0.516691</td>\n      <td>0.610018</td>\n      <td>13.7742</td>\n      <td>121.749</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>beto</td>\n      <td>es</td>\n      <td>1.094156</td>\n      <td>0.749370</td>\n      <td>0.781866</td>\n      <td>0.719468</td>\n      <td>0.671916</td>\n      <td>0.640000</td>\n      <td>0.707182</td>\n      <td>0.753695</td>\n      <td>...</td>\n      <td>0.121212</td>\n      <td>0.533333</td>\n      <td>0.444444</td>\n      <td>0.666667</td>\n      <td>0.547506</td>\n      <td>0.541121</td>\n      <td>0.568213</td>\n      <td>0.687537</td>\n      <td>10.6171</td>\n      <td>157.953</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 30 columns</p>\n</div>",
      "text/plain": "           Model Lang      Loss  Others f1  Others precision  Others recall  \\\n6  distilbert-en   en  1.378156   0.509407          0.633094       0.426150   \n3       mbert-en   en  1.386493   0.547718          0.638710       0.479419   \n5      bert-base   en  1.274540   0.593980          0.663677       0.537530   \n4   roberta-base   en  1.233089   0.573888          0.704225       0.484262   \n0  bertweet-base   en  1.172043   0.606019          0.696541       0.536320   \n7  distilbert-es   es  1.118434   0.678962          0.780220       0.600967   \n2       mbert-es   es  1.136714   0.691127          0.770833       0.626360   \n1           beto   es  1.094156   0.749370          0.781866       0.719468   \n\n     Joy f1  Joy precision  Joy recall  Sadness f1  ...  Disgust recall  \\\n6  0.665484       0.607780    0.735294    0.502165  ...        0.413613   \n3  0.650691       0.613913    0.692157    0.544643  ...        0.434555   \n5  0.683521       0.654122    0.715686    0.541485  ...        0.434555   \n4  0.689527       0.653779    0.729412    0.547945  ...        0.633508   \n0  0.711069       0.681655    0.743137    0.608696  ...        0.523560   \n7  0.602230       0.546067    0.671271    0.716707  ...        0.090909   \n2  0.585242       0.542453    0.635359    0.710462  ...        0.151515   \n1  0.671916       0.640000    0.707182    0.753695  ...        0.121212   \n\n    Fear f1  Fear precision  Fear recall  Macro f1  Macro precision  \\\n6  0.201835        0.154930     0.289474  0.382939         0.366219   \n3  0.255319        0.214286     0.315789  0.393619         0.379647   \n5  0.252632        0.210526     0.315789  0.439238         0.420341   \n4  0.255814        0.229167     0.289474  0.445122         0.431733   \n0  0.260000        0.209677     0.342105  0.475686         0.456330   \n7  0.461538        0.352941     0.666667  0.463233         0.437966   \n2  0.431373        0.333333     0.611111  0.473628         0.449092   \n1  0.533333        0.444444     0.666667  0.547506         0.541121   \n\n   Macro recall       Acc  Runtime  Samples per second  \n6      0.418844  0.503286   5.4081             337.642  \n3      0.419032  0.515882   9.2533             197.335  \n5      0.468907  0.559146   9.6085             190.041  \n4      0.490829  0.562979   9.5375             191.455  \n0      0.513214  0.584337   9.2916             196.522  \n7      0.512605  0.600477   7.9120             211.957  \n2      0.516691  0.610018  13.7742             121.749  \n1      0.568213  0.687537  10.6171             157.953  \n\n[8 rows x 30 columns]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {**evaluation, **evaluation[\"metrics\"]} for evaluation in evaluations\n",
    "])\n",
    "\n",
    "df.drop(labels=[\"predictions\", \"labels\", \"metrics\", \"file\"], inplace=True, axis=1)\n",
    "df[\"model\"] = df[\"model\"].str.replace(\"models/\", \"\")\n",
    "df[\"model\"] = df[\"model\"].str.replace(\"-emotion-analysis/\", \"\")\n",
    "df.columns = [col.replace(\"test_\", \"\").replace(\"_\", \" \").capitalize() for col in df.columns]\n",
    "#df.set_index(\"Model\", inplace=True)\n",
    "df = df.sort_values([\"Lang\", \"Macro f1\"]) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrrrr}\n",
      "\\toprule\n",
      "        Model & Lang &  Others f1 &  Joy f1 &  Sadness f1 &  Anger f1 &  Surprise f1 &  Disgust f1 &  Fear f1 &  Macro f1 \\\\\n",
      "\\midrule\n",
      "distilbert-en &   en &      0.509 &   0.665 &       0.502 &     0.303 &        0.148 &       0.351 &    0.202 &     0.383 \\\\\n",
      "     mbert-en &   en &      0.548 &   0.651 &       0.545 &     0.297 &        0.103 &       0.357 &    0.255 &     0.394 \\\\\n",
      "    bert-base &   en &      0.594 &   0.684 &       0.541 &     0.356 &        0.238 &       0.409 &    0.253 &     0.439 \\\\\n",
      " roberta-base &   en &      0.574 &   0.690 &       0.548 &     0.364 &        0.212 &       0.473 &    0.256 &     0.445 \\\\\n",
      "bertweet-base &   en &      0.606 &   0.711 &       0.609 &     0.434 &        0.258 &       0.452 &    0.260 &     0.476 \\\\\n",
      "distilbert-es &   es &      0.679 &   0.602 &       0.717 &     0.434 &        0.264 &       0.085 &    0.462 &     0.463 \\\\\n",
      "     mbert-es &   es &      0.691 &   0.585 &       0.710 &     0.491 &        0.273 &       0.133 &    0.431 &     0.474 \\\\\n",
      "         beto &   es &      0.749 &   0.672 &       0.754 &     0.574 &        0.397 &       0.154 &    0.533 &     0.548 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_columns = [col for col in df.columns if \"f1\" in col and \"Macro\" not in col]\n",
    "print(df[[\"Model\", \"Lang\"] + f1_columns + [\"Macro f1\"]].to_latex(index=False, float_format=\"{0:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         | Lang   |   Others f1 |   Joy f1 |   Sadness f1 |   Anger f1 |   Surprise f1 |   Disgust f1 |   Fear f1 |   Macro f1 |\n",
      "|:--------------|:-------|------------:|---------:|-------------:|-----------:|--------------:|-------------:|----------:|-----------:|\n",
      "| distilbert-en | en     |    0.509407 | 0.665484 |     0.502165 |   0.30303  |      0.147541 |     0.351111 |  0.201835 |   0.382939 |\n",
      "| mbert-en      | en     |    0.547718 | 0.650691 |     0.544643 |   0.29703  |      0.102941 |     0.356989 |  0.255319 |   0.393619 |\n",
      "| bert-base     | en     |    0.59398  | 0.683521 |     0.541485 |   0.355769 |      0.238411 |     0.408867 |  0.252632 |   0.439238 |\n",
      "| roberta-base  | en     |    0.573888 | 0.689527 |     0.547945 |   0.363636 |      0.212389 |     0.472656 |  0.255814 |   0.445122 |\n",
      "| bertweet-base | en     |    0.606019 | 0.711069 |     0.608696 |   0.433862 |      0.257669 |     0.452489 |  0.26     |   0.475686 |\n",
      "| distilbert-es | es     |    0.678962 | 0.60223  |     0.716707 |   0.434316 |      0.264368 |     0.084507 |  0.461538 |   0.463233 |\n",
      "| mbert-es      | es     |    0.691127 | 0.585242 |     0.710462 |   0.490566 |      0.273292 |     0.133333 |  0.431373 |   0.473628 |\n",
      "| beto          | es     |    0.74937  | 0.671916 |     0.753695 |   0.573684 |      0.396694 |     0.153846 |  0.533333 |   0.547506 |\n"
     ]
    }
   ],
   "source": [
    "f1_columns = [col for col in df.columns if \"f1\" in col and \"Macro\" not in col]\n",
    "print(df[[\"Model\", \"Lang\"] + f1_columns + [\"Macro f1\"]].to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pysent-oyXQVI9B': pipenv)",
   "name": "python385jvsc74a57bd01b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "1b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}