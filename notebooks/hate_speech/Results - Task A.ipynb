{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beto.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../../evaluations/hate_speech/task_b/no_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 beto evaluations\n",
      "We have 10 robertuito evaluations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beto mean</th>\n",
       "      <th>beto std</th>\n",
       "      <th>robertuito mean</th>\n",
       "      <th>robertuito std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hateful_f1</th>\n",
       "      <td>0.746965</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>0.780701</td>\n",
       "      <td>0.004079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hateful_precision</th>\n",
       "      <td>0.674079</td>\n",
       "      <td>0.020709</td>\n",
       "      <td>0.722964</td>\n",
       "      <td>0.015164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hateful_recall</th>\n",
       "      <td>0.838939</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.849242</td>\n",
       "      <td>0.020218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_f1</th>\n",
       "      <td>0.764005</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.801030</td>\n",
       "      <td>0.005535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>0.765437</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>0.803187</td>\n",
       "      <td>0.006134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   beto mean  beto std  robertuito mean  robertuito std\n",
       "hateful_f1          0.746965  0.007437         0.780701        0.004079\n",
       "hateful_precision   0.674079  0.020709         0.722964        0.015164\n",
       "hateful_recall      0.838939  0.025604         0.849242        0.020218\n",
       "macro_f1            0.764005  0.011425         0.801030        0.005535\n",
       "acc                 0.765437  0.012152         0.803187        0.006134"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "evaluations = {\n",
    "\n",
    "}\n",
    "\n",
    "for model_name, path in [\n",
    "    ('beto', '../../evaluations/hate_speech/task_a/beto.json'),\n",
    "    ('robertuito', '../../evaluations/hate_speech/task_a/robertuito.json'),\n",
    "    ]:\n",
    "    with open(path) as f:\n",
    "        evaluations[model_name] = json.load(f)\n",
    "\n",
    "\n",
    "for key, evals in evaluations.items():\n",
    "    print(f\"We have {len(evals['evaluations']['hate_speech'])} {key} evaluations\")\n",
    "\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model_name, model_results in evaluations.items():\n",
    "    model_evaluations = model_results[\"evaluations\"][\"hate_speech\"]\n",
    "    \n",
    "    if not model_evaluations:\n",
    "        continue\n",
    "    \n",
    "    df = pd.DataFrame(model_evaluations)\n",
    "\n",
    "    df.columns = [x.split(\"_\", 1)[1] if \"_\" in x else x for x in df.columns]\n",
    "    \n",
    "    mean_df = pd.DataFrame({\n",
    "        f\"{model_name} mean\": df.mean(), \n",
    "        f\"{model_name} std\": df.std()\n",
    "    })\n",
    "    dfs.append(mean_df)\n",
    "\n",
    "result_df = pd.concat(dfs, axis=1)\n",
    "index = [\n",
    "    \"hateful_f1\",\n",
    "    \"hateful_precision\",\n",
    "    \"hateful_recall\",\n",
    "    \"macro_f1\",\n",
    "    \"acc\",\n",
    "]\n",
    "result_df.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=73.5, pvalue=0.04093431186532218)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "hier_emr = [evaluation[\"eval_emr\"] for evaluation in evaluations[\"beto-hier\"][\"evaluations\"][\"hate_speech\"]]\n",
    "standard_emr = [evaluation[\"eval_emr\"] for evaluation in evaluations[\"beto\"][\"evaluations\"][\"hate_speech\"]]\n",
    "\n",
    "scipy.stats.mannwhitneyu(hier_emr, standard_emr, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=68.0, pvalue=0.09269007167775994)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "hier_emr = [evaluation[\"eval_emr\"] for evaluation in evaluations[\"robertuito-hier-zero\"][\"evaluations\"][\"hate_speech\"]]\n",
    "standard_emr = [evaluation[\"eval_emr\"] for evaluation in evaluations[\"robertuito\"][\"evaluations\"][\"hate_speech\"]]\n",
    "\n",
    "scipy.stats.mannwhitneyu(hier_emr, standard_emr, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('pysentimiento-bwlKzHxB-py3.7': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
