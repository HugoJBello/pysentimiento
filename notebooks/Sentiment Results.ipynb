{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../evaluations/sentiment_beto.json\n",
      "../evaluations/sentiment_roberta_base.json\n",
      "../evaluations/sentiment_mbert_en.json\n",
      "../evaluations/sentiment_bert_base.json\n",
      "../evaluations/sentiment_distilbert_es.json\n",
      "../evaluations/sentiment_bertweet_base.json\n",
      "../evaluations/sentiment_mbert_es.json\n",
      "../evaluations/sentiment_distilbert_en.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob \n",
    "import json \n",
    "\n",
    "evaluation_files = glob.glob(\"../evaluations/sentiment_*.json\")\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for file in evaluation_files:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        evaluation = json.load(f)\n",
    "        evaluation[\"file\"] = file.split(\"/\")[-1]\n",
    "        evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Lang</th>\n      <th>Model</th>\n      <th>Loss</th>\n      <th>Neg f1</th>\n      <th>Neg precision</th>\n      <th>Neg recall</th>\n      <th>Neu f1</th>\n      <th>Neu precision</th>\n      <th>Neu recall</th>\n      <th>Pos f1</th>\n      <th>Pos precision</th>\n      <th>Pos recall</th>\n      <th>Macro f1</th>\n      <th>Macro precision</th>\n      <th>Macro recall</th>\n      <th>Acc</th>\n      <th>Runtime</th>\n      <th>Samples per second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>en</td>\n      <td>distilbert-en</td>\n      <td>0.763219</td>\n      <td>0.622820</td>\n      <td>0.660921</td>\n      <td>0.588872</td>\n      <td>0.672543</td>\n      <td>0.658175</td>\n      <td>0.687553</td>\n      <td>0.629931</td>\n      <td>0.609123</td>\n      <td>0.652211</td>\n      <td>0.641765</td>\n      <td>0.642740</td>\n      <td>0.642878</td>\n      <td>0.648811</td>\n      <td>42.2713</td>\n      <td>290.599</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>en</td>\n      <td>mbert-en</td>\n      <td>0.855557</td>\n      <td>0.665551</td>\n      <td>0.597556</td>\n      <td>0.751007</td>\n      <td>0.634016</td>\n      <td>0.697533</td>\n      <td>0.581102</td>\n      <td>0.629104</td>\n      <td>0.632992</td>\n      <td>0.625263</td>\n      <td>0.642891</td>\n      <td>0.642694</td>\n      <td>0.652457</td>\n      <td>0.644578</td>\n      <td>63.8805</td>\n      <td>192.297</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>en</td>\n      <td>roberta-base</td>\n      <td>1.206942</td>\n      <td>0.702262</td>\n      <td>0.689731</td>\n      <td>0.715257</td>\n      <td>0.682952</td>\n      <td>0.703050</td>\n      <td>0.663972</td>\n      <td>0.665721</td>\n      <td>0.641908</td>\n      <td>0.691368</td>\n      <td>0.683645</td>\n      <td>0.678229</td>\n      <td>0.690199</td>\n      <td>0.685852</td>\n      <td>62.7816</td>\n      <td>195.662</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>en</td>\n      <td>bert-base</td>\n      <td>0.719900</td>\n      <td>0.693249</td>\n      <td>0.726220</td>\n      <td>0.663142</td>\n      <td>0.688558</td>\n      <td>0.703125</td>\n      <td>0.674583</td>\n      <td>0.670165</td>\n      <td>0.603850</td>\n      <td>0.752842</td>\n      <td>0.683991</td>\n      <td>0.677732</td>\n      <td>0.696856</td>\n      <td>0.686014</td>\n      <td>62.1945</td>\n      <td>197.509</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>en</td>\n      <td>bertweet-base</td>\n      <td>1.236025</td>\n      <td>0.733372</td>\n      <td>0.680379</td>\n      <td>0.795317</td>\n      <td>0.677449</td>\n      <td>0.730657</td>\n      <td>0.631464</td>\n      <td>0.678403</td>\n      <td>0.660159</td>\n      <td>0.697684</td>\n      <td>0.696408</td>\n      <td>0.690398</td>\n      <td>0.708155</td>\n      <td>0.697248</td>\n      <td>64.9213</td>\n      <td>189.214</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>es</td>\n      <td>distilbert-es</td>\n      <td>0.975005</td>\n      <td>0.649262</td>\n      <td>0.689484</td>\n      <td>0.613474</td>\n      <td>0.479947</td>\n      <td>0.421640</td>\n      <td>0.556968</td>\n      <td>0.667876</td>\n      <td>0.717836</td>\n      <td>0.624417</td>\n      <td>0.599028</td>\n      <td>0.609653</td>\n      <td>0.598286</td>\n      <td>0.601735</td>\n      <td>23.6880</td>\n      <td>306.653</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>es</td>\n      <td>mbert-es</td>\n      <td>1.359172</td>\n      <td>0.667889</td>\n      <td>0.724344</td>\n      <td>0.619599</td>\n      <td>0.475637</td>\n      <td>0.424242</td>\n      <td>0.541200</td>\n      <td>0.669420</td>\n      <td>0.686887</td>\n      <td>0.652819</td>\n      <td>0.604315</td>\n      <td>0.611824</td>\n      <td>0.604539</td>\n      <td>0.609169</td>\n      <td>37.1661</td>\n      <td>195.447</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>es</td>\n      <td>beto</td>\n      <td>1.462894</td>\n      <td>0.728972</td>\n      <td>0.772571</td>\n      <td>0.690031</td>\n      <td>0.535633</td>\n      <td>0.483607</td>\n      <td>0.600203</td>\n      <td>0.734971</td>\n      <td>0.761710</td>\n      <td>0.710047</td>\n      <td>0.666526</td>\n      <td>0.672629</td>\n      <td>0.666760</td>\n      <td>0.672219</td>\n      <td>41.9681</td>\n      <td>173.084</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  Lang          Model      Loss    Neg f1  Neg precision  Neg recall  \\\n7   en  distilbert-en  0.763219  0.622820       0.660921    0.588872   \n2   en       mbert-en  0.855557  0.665551       0.597556    0.751007   \n1   en   roberta-base  1.206942  0.702262       0.689731    0.715257   \n3   en      bert-base  0.719900  0.693249       0.726220    0.663142   \n5   en  bertweet-base  1.236025  0.733372       0.680379    0.795317   \n4   es  distilbert-es  0.975005  0.649262       0.689484    0.613474   \n6   es       mbert-es  1.359172  0.667889       0.724344    0.619599   \n0   es           beto  1.462894  0.728972       0.772571    0.690031   \n\n     Neu f1  Neu precision  Neu recall    Pos f1  Pos precision  Pos recall  \\\n7  0.672543       0.658175    0.687553  0.629931       0.609123    0.652211   \n2  0.634016       0.697533    0.581102  0.629104       0.632992    0.625263   \n1  0.682952       0.703050    0.663972  0.665721       0.641908    0.691368   \n3  0.688558       0.703125    0.674583  0.670165       0.603850    0.752842   \n5  0.677449       0.730657    0.631464  0.678403       0.660159    0.697684   \n4  0.479947       0.421640    0.556968  0.667876       0.717836    0.624417   \n6  0.475637       0.424242    0.541200  0.669420       0.686887    0.652819   \n0  0.535633       0.483607    0.600203  0.734971       0.761710    0.710047   \n\n   Macro f1  Macro precision  Macro recall       Acc  Runtime  \\\n7  0.641765         0.642740      0.642878  0.648811  42.2713   \n2  0.642891         0.642694      0.652457  0.644578  63.8805   \n1  0.683645         0.678229      0.690199  0.685852  62.7816   \n3  0.683991         0.677732      0.696856  0.686014  62.1945   \n5  0.696408         0.690398      0.708155  0.697248  64.9213   \n4  0.599028         0.609653      0.598286  0.601735  23.6880   \n6  0.604315         0.611824      0.604539  0.609169  37.1661   \n0  0.666526         0.672629      0.666760  0.672219  41.9681   \n\n   Samples per second  \n7             290.599  \n2             192.297  \n1             195.662  \n3             197.509  \n5             189.214  \n4             306.653  \n6             195.447  \n0             173.084  "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {**evaluation, **evaluation[\"metrics\"]} for evaluation in evaluations\n",
    "])\n",
    "\n",
    "df.drop(labels=[\"predictions\", \"labels\", \"metrics\", \"file\"], inplace=True, axis=1)\n",
    "df[\"model\"] = df[\"model\"].str.replace(\"models/\", \"\")\n",
    "df[\"model\"] = df[\"model\"].str.replace(\"-sentiment-analysis/\", \"\")\n",
    "df.columns = [col.replace(\"test_\", \"\").replace(\"_\", \" \").capitalize() for col in df.columns]\n",
    "#df.set_index(\"Model\", inplace=True)\n",
    "df = df.sort_values([\"Lang\", \"Macro f1\"]) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      "        Model & Lang &  Neg f1 &  Pos f1 &  Neu f1 &  Macro f1 \\\\\n",
      "\\midrule\n",
      "distilbert-en &   en &   0.623 &   0.630 &   0.673 &     0.642 \\\\\n",
      "     mbert-en &   en &   0.666 &   0.629 &   0.634 &     0.643 \\\\\n",
      " roberta-base &   en &   0.702 &   0.666 &   0.683 &     0.684 \\\\\n",
      "    bert-base &   en &   0.693 &   0.670 &   0.689 &     0.684 \\\\\n",
      "bertweet-base &   en &   0.733 &   0.678 &   0.677 &     0.696 \\\\\n",
      "distilbert-es &   es &   0.649 &   0.668 &   0.480 &     0.599 \\\\\n",
      "     mbert-es &   es &   0.668 &   0.669 &   0.476 &     0.604 \\\\\n",
      "         beto &   es &   0.729 &   0.735 &   0.536 &     0.667 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"Model\", \"Lang\", \"Neg f1\", \"Pos f1\", \"Neu f1\", \"Macro f1\"]].to_latex(index=False, float_format=\"{0:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         | Lang   |   Neg f1 |   Pos f1 |   Neu f1 |   Macro f1 |\n",
      "|:--------------|:-------|---------:|---------:|---------:|-----------:|\n",
      "| distilbert-en | en     | 0.62282  | 0.629931 | 0.672543 |   0.641765 |\n",
      "| mbert-en      | en     | 0.665551 | 0.629104 | 0.634016 |   0.642891 |\n",
      "| roberta-base  | en     | 0.702262 | 0.665721 | 0.682952 |   0.683645 |\n",
      "| bert-base     | en     | 0.693249 | 0.670165 | 0.688558 |   0.683991 |\n",
      "| bertweet-base | en     | 0.733372 | 0.678403 | 0.677449 |   0.696408 |\n",
      "| distilbert-es | es     | 0.649262 | 0.667876 | 0.479947 |   0.599028 |\n",
      "| mbert-es      | es     | 0.667889 | 0.66942  | 0.475637 |   0.604315 |\n",
      "| beto          | es     | 0.728972 | 0.734971 | 0.535633 |   0.666526 |\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    df[[\"Model\", \"Lang\", \"Neg f1\", \"Pos f1\", \"Neu f1\", \"Macro f1\"]].to_markdown(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pysent-oyXQVI9B': pipenv)",
   "name": "python385jvsc74a57bd01b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}