{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In this notebook we show the results of our experiments for both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "beto_cased.json    birnn_twitter.json  mbert_uncased.json  roberta.json\n",
      "beto_uncased.json  ffn_cc.json\t       rnn_cc.json\n",
      "birnn_cc.json\t   ffn_twitter.json    rnn_twitter.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "base_path = \"../evaluations/es\"\n",
    "!ls $base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../evaluations/es/birnn_twitter.json\n",
      "../evaluations/es/ffn_twitter.json\n",
      "../evaluations/es/roberta.json\n",
      "../evaluations/es/beto_uncased.json\n",
      "../evaluations/es/mbert_uncased.json\n",
      "../evaluations/es/birnn_cc.json\n",
      "../evaluations/es/rnn_twitter.json\n",
      "../evaluations/es/beto_cased.json\n",
      "../evaluations/es/rnn_cc.json\n",
      "../evaluations/es/ffn_cc.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['birnn_twitter', 'ffn_twitter', 'roberta', 'beto_uncased', 'mbert_uncased', 'birnn_cc', 'rnn_twitter', 'beto_cased', 'rnn_cc', 'ffn_cc'])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def clean_key(k):\n",
    "    return k.split(\"_\", 1)[1]\n",
    "\n",
    "\n",
    "evaluation_paths = glob.glob(f\"{base_path}/*.json\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "for path in evaluation_paths:\n",
    "    print(path)\n",
    "    name = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path) as f:\n",
    "        model_evaluation = json.load(f)\n",
    "        clean_evaluations = []\n",
    "        for task in model_evaluation[\"evaluations\"].keys():\n",
    "            task_evaluations = model_evaluation[\"evaluations\"][task]\n",
    "            clean_evaluations = [\n",
    "                {clean_key(k): v for k, v in ev.items()} \n",
    "                for ev in task_evaluations\n",
    "            ]\n",
    "\n",
    "            model_evaluation[\"evaluations\"][task] = clean_evaluations\n",
    "        models[name] = model_evaluation\n",
    "        \n",
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "birnn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "ffn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "roberta\n",
      "hate_speech\n",
      "3\n",
      "sentiment\n",
      "3\n",
      "emotion\n",
      "2\n",
      "irony\n",
      "2\n",
      "==================================================\n",
      "beto_uncased\n",
      "hate_speech\n",
      "9\n",
      "sentiment\n",
      "9\n",
      "emotion\n",
      "9\n",
      "irony\n",
      "9\n",
      "==================================================\n",
      "mbert_uncased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "birnn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "rnn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "beto_cased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "rnn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "ffn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"hate_speech\": \"macro_f1\",\n",
    "    \"sentiment\": \"macro_f1\",\n",
    "    \"emotion\": \"macro_f1\",\n",
    "    \"irony\": \"macro_f1\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, model_evaluation in models.items():\n",
    "    print(\"=\"*50)\n",
    "    print(model)\n",
    "    for task, task_evaluations in model_evaluation[\"evaluations\"].items():\n",
    "        print(task)\n",
    "        print(len(task_evaluations))\n",
    "        for evaluation in task_evaluations:\n",
    "            metric = metrics[task]\n",
    "\n",
    "            ## TODO\n",
    "            if metric not in evaluation:\n",
    "                metric = \"hateful_f1\"\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"task\": task,\n",
    "                \"metric\": evaluation[metric],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>emotion</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>irony</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.543044</td>\n",
       "      <td>0.752739</td>\n",
       "      <td>0.725732</td>\n",
       "      <td>0.665281</td>\n",
       "      <td>0.671699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_uncased</th>\n",
       "      <td>0.532597</td>\n",
       "      <td>0.726768</td>\n",
       "      <td>0.700017</td>\n",
       "      <td>0.650497</td>\n",
       "      <td>0.652469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_cased</th>\n",
       "      <td>0.515775</td>\n",
       "      <td>0.724333</td>\n",
       "      <td>0.705045</td>\n",
       "      <td>0.661804</td>\n",
       "      <td>0.651739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_uncased</th>\n",
       "      <td>0.492690</td>\n",
       "      <td>0.718191</td>\n",
       "      <td>0.681166</td>\n",
       "      <td>0.617423</td>\n",
       "      <td>0.627368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_twitter</th>\n",
       "      <td>0.263644</td>\n",
       "      <td>0.592427</td>\n",
       "      <td>0.631280</td>\n",
       "      <td>0.584750</td>\n",
       "      <td>0.518025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_twitter</th>\n",
       "      <td>0.269254</td>\n",
       "      <td>0.538498</td>\n",
       "      <td>0.627983</td>\n",
       "      <td>0.601973</td>\n",
       "      <td>0.509427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_cc</th>\n",
       "      <td>0.231306</td>\n",
       "      <td>0.534061</td>\n",
       "      <td>0.624745</td>\n",
       "      <td>0.553016</td>\n",
       "      <td>0.485782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_cc</th>\n",
       "      <td>0.236841</td>\n",
       "      <td>0.515943</td>\n",
       "      <td>0.580589</td>\n",
       "      <td>0.564412</td>\n",
       "      <td>0.474446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_twitter</th>\n",
       "      <td>0.202616</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.627103</td>\n",
       "      <td>0.516410</td>\n",
       "      <td>0.432553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_cc</th>\n",
       "      <td>0.179283</td>\n",
       "      <td>0.402603</td>\n",
       "      <td>0.481485</td>\n",
       "      <td>0.509497</td>\n",
       "      <td>0.393217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task            emotion  hate_speech     irony  sentiment     score\n",
       "model                                                              \n",
       "roberta        0.543044     0.752739  0.725732   0.665281  0.671699\n",
       "beto_uncased   0.532597     0.726768  0.700017   0.650497  0.652469\n",
       "beto_cased     0.515775     0.724333  0.705045   0.661804  0.651739\n",
       "mbert_uncased  0.492690     0.718191  0.681166   0.617423  0.627368\n",
       "birnn_twitter  0.263644     0.592427  0.631280   0.584750  0.518025\n",
       "rnn_twitter    0.269254     0.538498  0.627983   0.601973  0.509427\n",
       "birnn_cc       0.231306     0.534061  0.624745   0.553016  0.485782\n",
       "rnn_cc         0.236841     0.515943  0.580589   0.564412  0.474446\n",
       "ffn_twitter    0.202616     0.384085  0.627103   0.516410  0.432553\n",
       "ffn_cc         0.179283     0.402603  0.481485   0.509497  0.393217"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_df = pd.DataFrame(results).groupby([\"model\", \"task\"]).mean().stack()\n",
    "std_df = pd.DataFrame(results).groupby([\"model\", \"task\"]).mean().stack()\n",
    "# Magia negra\n",
    "mean_df.index = mean_df.index.droplevel(-1)\n",
    "std_df.index = std_df.index.droplevel(-1)\n",
    "\n",
    "mean_df = mean_df.unstack(1)\n",
    "std_df = std_df.unstack(1)\n",
    "\n",
    "mean_df[\"score\"] = mean_df.mean(1)\n",
    "\n",
    "mean_df.sort_values(\"score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "ffn_twitter\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "===========\n",
      "mbert_uncased\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "===========\n",
      "birnn_cc\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "===========\n",
      "rnn_twitter\n",
      "10\n",
      "10\n",
      "10\n",
      "9\n",
      "===========\n",
      "beto_cased\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "===========\n",
      "rnn_cc\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "===========\n",
      "ffn_cc\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    print(\"===========\")\n",
    "    print(f\"{model_name}\")\n",
    "    for task, task_runs in model_info[\"evaluations\"].items():\n",
    "        print(len(task_runs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "../evaluations/sentiment_beto.json\n",
      "../evaluations/sentiment_roberta_base.json\n",
      "../evaluations/sentiment_mbert_en.json\n",
      "../evaluations/sentiment_bert_base.json\n",
      "../evaluations/sentiment_distilbert_es.json\n",
      "../evaluations/sentiment_bertweet_base.json\n",
      "../evaluations/sentiment_mbert_es.json\n",
      "../evaluations/sentiment_distilbert_en.json\n",
      "../evaluations/emotion_bertweet_base.json\n",
      "../evaluations/emotion_beto.json\n",
      "../evaluations/emotion_mbert_es.json\n",
      "../evaluations/emotion_mbert_en.json\n",
      "../evaluations/emotion_roberta.json\n",
      "../evaluations/emotion_bert_base.json\n",
      "../evaluations/emotion_distilbert_en.json\n",
      "../evaluations/emotion_distilbert_es.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob \n",
    "import json \n",
    "\n",
    "tasks = [\"sentiment\", \"emotion\"]\n",
    "\n",
    "files = [(task, f) for task in tasks for f in glob.glob(f\"../evaluations/{task}_*.json\")]\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for task, file in files:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        evaluation = json.load(f)\n",
    "        evaluation[\"task\"] = task\n",
    "        evaluation[\"file\"] = file.split(\"/\")[-1]\n",
    "        evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for task in tasks:\n",
    "    df =  pd.DataFrame([\n",
    "        {**evaluation, **evaluation[\"metrics\"]} for evaluation in evaluations if evaluation[\"task\"] == task\n",
    "    ])\n",
    "\n",
    "    df.drop(labels=[\"predictions\", \"labels\", \"metrics\", \"file\"], inplace=True, axis=1)\n",
    "    df[\"model\"] = df[\"model\"].str.replace(\"models/\", \"\")\n",
    "\n",
    "    df[\"model\"] = df[\"model\"].str.replace(f\"-{task}-analysis/\", \"\")\n",
    "    df.columns = [col.replace(\"test_\", \"\").replace(\"_\", \" \").capitalize() for col in df.columns]\n",
    "    #df.set_index(\"Model\", inplace=True)\n",
    "    df = df.sort_values([\"Lang\", \"Macro f1\"]) \n",
    "    dfs[task] = df\n",
    "\n",
    "df = dfs[\"sentiment\"].merge(dfs[\"emotion\"], on=\"Model\", suffixes=(\"\", \"_emotion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lang', 'Model', 'Task', 'Loss', 'Neg f1', 'Neg precision',\n",
       "       'Neg recall', 'Neu f1', 'Neu precision', 'Neu recall', 'Pos f1',\n",
       "       'Pos precision', 'Pos recall', 'Micro f1', 'Macro f1',\n",
       "       'Macro precision', 'Macro recall', 'Acc', 'Runtime',\n",
       "       'Samples per second', 'Lang_emotion', 'Task_emotion', 'Loss_emotion',\n",
       "       'Others f1', 'Others precision', 'Others recall', 'Joy f1',\n",
       "       'Joy precision', 'Joy recall', 'Sadness f1', 'Sadness precision',\n",
       "       'Sadness recall', 'Anger f1', 'Anger precision', 'Anger recall',\n",
       "       'Surprise f1', 'Surprise precision', 'Surprise recall', 'Disgust f1',\n",
       "       'Disgust precision', 'Disgust recall', 'Fear f1', 'Fear precision',\n",
       "       'Fear recall', 'Micro f1_emotion', 'Macro f1_emotion',\n",
       "       'Macro precision_emotion', 'Macro recall_emotion', 'Acc_emotion',\n",
       "       'Runtime_emotion', 'Samples per second_emotion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      "        Model &         Lang &     Micro f1 &     Macro f1 &  Micro f1\\_emotion &  Macro f1\\_emotion \\\\\n",
      "\\midrule\n",
      "distilbert-en &           en &        0.649 &        0.642 &             0.503 &             0.383 \\\\\n",
      "     mbert-en &           en &        0.645 &        0.643 &             0.516 &             0.394 \\\\\n",
      " roberta-base &           en &        0.686 &        0.684 &             0.563 &             0.445 \\\\\n",
      "    bert-base &           en &        0.686 &        0.684 &             0.559 &             0.439 \\\\\n",
      "bertweet-base &           en &        0.697 &        0.696 &             0.584 &             0.476 \\\\\n",
      "distilbert-es &           es &        0.602 &        0.599 &             0.600 &             0.463 \\\\\n",
      "     mbert-es &           es &        0.609 &        0.604 &             0.610 &             0.474 \\\\\n",
      "         beto &           es &        0.672 &        0.667 &             0.688 &             0.548 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_columns = [\"Model\", \"Lang\"]\n",
    "\n",
    "df.sort_values([\"Lang\", \"Macro f1\"], inplace=True) \n",
    "sentiment_columns = [\n",
    "    #\"Pos f1\", \n",
    "    #\"Neu f1\", \n",
    "    #\"Neg f1\", \n",
    "    \"Micro f1\",\n",
    "    \"Macro f1\"\n",
    "]\n",
    "emotion_columns = [\n",
    "    #\"Joy f1\",\n",
    "    #\"Others f1\",\n",
    "    #\"Sadness f1\",\n",
    "    #\"Anger f1\",\n",
    "    #\"Disgust f1\",\n",
    "    \"Micro f1_emotion\",\n",
    "    \"Macro f1_emotion\"\n",
    "]\n",
    "\n",
    "print(df[base_columns + sentiment_columns + emotion_columns].to_latex(index=False, float_format=\"{0:.3f}\".format, col_space=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         | Lang   |   Others f1 |   Joy f1 |   Sadness f1 |   Anger f1 |   Surprise f1 |   Disgust f1 |   Fear f1 |   Macro f1 |\n",
      "|:--------------|:-------|------------:|---------:|-------------:|-----------:|--------------:|-------------:|----------:|-----------:|\n",
      "| distilbert-en | en     |    0.509407 | 0.665484 |     0.502165 |   0.30303  |      0.147541 |     0.351111 |  0.201835 |   0.382939 |\n",
      "| mbert-en      | en     |    0.547718 | 0.650691 |     0.544643 |   0.29703  |      0.102941 |     0.356989 |  0.255319 |   0.393619 |\n",
      "| bert-base     | en     |    0.59398  | 0.683521 |     0.541485 |   0.355769 |      0.238411 |     0.408867 |  0.252632 |   0.439238 |\n",
      "| roberta-base  | en     |    0.573888 | 0.689527 |     0.547945 |   0.363636 |      0.212389 |     0.472656 |  0.255814 |   0.445122 |\n",
      "| bertweet-base | en     |    0.606019 | 0.711069 |     0.608696 |   0.433862 |      0.257669 |     0.452489 |  0.26     |   0.475686 |\n",
      "| distilbert-es | es     |    0.678962 | 0.60223  |     0.716707 |   0.434316 |      0.264368 |     0.084507 |  0.461538 |   0.463233 |\n",
      "| mbert-es      | es     |    0.691127 | 0.585242 |     0.710462 |   0.490566 |      0.273292 |     0.133333 |  0.431373 |   0.473628 |\n",
      "| beto          | es     |    0.74937  | 0.671916 |     0.753695 |   0.573684 |      0.396694 |     0.153846 |  0.533333 |   0.547506 |\n"
     ]
    }
   ],
   "source": [
    "f1_columns = [col for col in df.columns if \"f1\" in col and \"Macro\" not in col]\n",
    "print(df[[\"Model\", \"Lang\"] + f1_columns + [\"Macro f1\"]].to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pysent-oyXQVI9B': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
