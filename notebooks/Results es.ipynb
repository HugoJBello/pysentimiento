{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In this notebook we show the results of our experiments for both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "bertin.json\t   birnn_cc.json       ffn_twitter.json    rnn_twitter.json\n",
      "beto_cased.json    birnn_twitter.json  mbert_uncased.json  roberta.json\n",
      "beto_uncased.json  ffn_cc.json\t       rnn_cc.json\t   robertuito.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "base_path = \"../evaluations/es\"\n",
    "!ls $base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../evaluations/es/birnn_twitter.json\n",
      "../evaluations/es/ffn_twitter.json\n",
      "../evaluations/es/roberta.json\n",
      "../evaluations/es/beto_uncased.json\n",
      "../evaluations/es/mbert_uncased.json\n",
      "../evaluations/es/birnn_cc.json\n",
      "../evaluations/es/robertuito.json\n",
      "../evaluations/es/bertin.json\n",
      "../evaluations/es/rnn_twitter.json\n",
      "../evaluations/es/beto_cased.json\n",
      "../evaluations/es/rnn_cc.json\n",
      "../evaluations/es/ffn_cc.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['birnn_twitter', 'ffn_twitter', 'roberta', 'beto_uncased', 'mbert_uncased', 'birnn_cc', 'robertuito', 'bertin', 'rnn_twitter', 'beto_cased', 'rnn_cc', 'ffn_cc'])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def clean_key(k):\n",
    "    return k.split(\"_\", 1)[1]\n",
    "\n",
    "\n",
    "evaluation_paths = glob.glob(f\"{base_path}/*.json\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "for path in evaluation_paths:\n",
    "    print(path)\n",
    "    name = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path) as f:\n",
    "        model_evaluation = json.load(f)\n",
    "        clean_evaluations = []\n",
    "        for task in model_evaluation[\"evaluations\"].keys():\n",
    "            task_evaluations = model_evaluation[\"evaluations\"][task]\n",
    "            clean_evaluations = [\n",
    "                {clean_key(k): v for k, v in ev.items()} \n",
    "                for ev in task_evaluations\n",
    "            ]\n",
    "\n",
    "            model_evaluation[\"evaluations\"][task] = clean_evaluations\n",
    "        models[name] = model_evaluation\n",
    "        \n",
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "birnn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "ffn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "roberta\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "beto_uncased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "mbert_uncased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "birnn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "robertuito\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "bertin\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "9\n",
      "==================================================\n",
      "rnn_twitter\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "beto_cased\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "rnn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n",
      "==================================================\n",
      "ffn_cc\n",
      "hate_speech\n",
      "10\n",
      "sentiment\n",
      "10\n",
      "emotion\n",
      "10\n",
      "irony\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"hate_speech\": \"macro_f1\",\n",
    "    \"sentiment\": \"macro_f1\",\n",
    "    \"emotion\": \"macro_f1\",\n",
    "    \"irony\": \"macro_f1\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model, model_evaluation in models.items():\n",
    "    print(\"=\"*50)\n",
    "    print(model)\n",
    "    for task, task_evaluations in model_evaluation[\"evaluations\"].items():\n",
    "        print(task)\n",
    "        print(len(task_evaluations))\n",
    "        for evaluation in task_evaluations:\n",
    "            metric = metrics[task]\n",
    "\n",
    "            ## TODO\n",
    "            if metric not in evaluation:\n",
    "                metric = \"hateful_f1\"\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"task\": task,\n",
    "                \"metric\": evaluation[metric],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>emotion</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>irony</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>robertuito</th>\n",
       "      <td>0.560</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.527</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertin</th>\n",
       "      <td>0.524</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_uncased</th>\n",
       "      <td>0.532</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beto_cased</th>\n",
       "      <td>0.516</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbert_uncased</th>\n",
       "      <td>0.493</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_twitter</th>\n",
       "      <td>0.264</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_twitter</th>\n",
       "      <td>0.269</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birnn_cc</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_cc</th>\n",
       "      <td>0.237</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_twitter</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffn_cc</th>\n",
       "      <td>0.179</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task           emotion  hate_speech  irony  sentiment  score\n",
       "model                                                       \n",
       "robertuito       0.560        0.759  0.739      0.705  0.691\n",
       "roberta          0.527        0.741  0.721      0.670  0.665\n",
       "bertin           0.524        0.738  0.713      0.666  0.660\n",
       "beto_uncased     0.532        0.727  0.701      0.651  0.653\n",
       "beto_cased       0.516        0.724  0.705      0.662  0.652\n",
       "mbert_uncased    0.493        0.718  0.681      0.617  0.627\n",
       "birnn_twitter    0.264        0.592  0.631      0.585  0.518\n",
       "rnn_twitter      0.269        0.538  0.628      0.602  0.509\n",
       "birnn_cc         0.231        0.534  0.625      0.553  0.486\n",
       "rnn_cc           0.237        0.516  0.581      0.564  0.474\n",
       "ffn_twitter      0.203        0.384  0.627      0.516  0.433\n",
       "ffn_cc           0.179        0.403  0.481      0.509  0.393"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "mean_df = pd.DataFrame(results).groupby([\"model\", \"task\"]).mean().stack()\n",
    "std_df = pd.DataFrame(results).groupby([\"model\", \"task\"]).mean().stack()\n",
    "# Magia negra\n",
    "mean_df.index = mean_df.index.droplevel(-1)\n",
    "std_df.index = std_df.index.droplevel(-1)\n",
    "\n",
    "mean_df = mean_df.unstack(1)\n",
    "std_df = std_df.unstack(1)\n",
    "\n",
    "mean_df[\"score\"] = mean_df.mean(1)\n",
    "\n",
    "mean_df.sort_values(\"score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_space\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mna_rep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NaN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mformatters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfloat_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EngFormatter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msparsify\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mindex_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mjustify\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_rows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_rows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshow_dimensions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mline_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_colwidth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Render a DataFrame to a console-friendly tabular output.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "buf : str, Path or StringIO-like, optional, default None\n",
      "    Buffer to write to. If None, the output is returned as a string.\n",
      "columns : sequence, optional, default None\n",
      "    The subset of columns to write. Writes all columns by default.\n",
      "col_space : int, list or dict of int, optional\n",
      "    The minimum width of each column.\n",
      "header : bool or sequence, optional\n",
      "    Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names.\n",
      "index : bool, optional, default True\n",
      "    Whether to print index (row) labels.\n",
      "na_rep : str, optional, default 'NaN'\n",
      "    String representation of NAN to use.\n",
      "formatters : list, tuple or dict of one-param. functions, optional\n",
      "    Formatter functions to apply to columns' elements by position or\n",
      "    name.\n",
      "    The result of each function must be a unicode string.\n",
      "    List/tuple must be of length equal to the number of columns.\n",
      "float_format : one-parameter function, optional, default None\n",
      "    Formatter function to apply to columns' elements if they are\n",
      "    floats. The result of this function must be a unicode string.\n",
      "sparsify : bool, optional, default True\n",
      "    Set to False for a DataFrame with a hierarchical index to print\n",
      "    every multiindex key at each row.\n",
      "index_names : bool, optional, default True\n",
      "    Prints the names of the indexes.\n",
      "justify : str, default None\n",
      "    How to justify the column labels. If None uses the option from\n",
      "    the print configuration (controlled by set_option), 'right' out\n",
      "    of the box. Valid values are\n",
      "\n",
      "    * left\n",
      "    * right\n",
      "    * center\n",
      "    * justify\n",
      "    * justify-all\n",
      "    * start\n",
      "    * end\n",
      "    * inherit\n",
      "    * match-parent\n",
      "    * initial\n",
      "    * unset.\n",
      "max_rows : int, optional\n",
      "    Maximum number of rows to display in the console.\n",
      "min_rows : int, optional\n",
      "    The number of rows to display in the console in a truncated repr\n",
      "    (when number of rows is above `max_rows`).\n",
      "max_cols : int, optional\n",
      "    Maximum number of columns to display in the console.\n",
      "show_dimensions : bool, default False\n",
      "    Display DataFrame dimensions (number of rows by number of columns).\n",
      "decimal : str, default '.'\n",
      "    Character recognized as decimal separator, e.g. ',' in Europe.\n",
      "\n",
      "line_width : int, optional\n",
      "    Width to wrap a line in characters.\n",
      "max_colwidth : int, optional\n",
      "    Max width to truncate each column in characters. By default, no limit.\n",
      "\n",
      "    .. versionadded:: 1.0.0\n",
      "encoding : str, default \"utf-8\"\n",
      "    Set character encoding.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "str or None\n",
      "    If buf is None, returns the result as a string. Otherwise returns\n",
      "    None.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "to_html : Convert DataFrame to HTML.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n",
      ">>> df = pd.DataFrame(d)\n",
      ">>> print(df.to_string())\n",
      "   col1  col2\n",
      "0     1     4\n",
      "1     2     5\n",
      "2     3     6\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.cache/pypoetry/virtualenvs/pysentimiento-bwlKzHxB-py3.7/lib/python3.7/site-packages/pandas/core/frame.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "mean_df.to_string("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "ffn_twitter\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "===========\n",
      "mbert_uncased\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "===========\n",
      "birnn_cc\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "===========\n",
      "rnn_twitter\n",
      "10\n",
      "10\n",
      "10\n",
      "9\n",
      "===========\n",
      "beto_cased\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "===========\n",
      "rnn_cc\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "===========\n",
      "ffn_cc\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    print(\"===========\")\n",
    "    print(f\"{model_name}\")\n",
    "    for task, task_runs in model_info[\"evaluations\"].items():\n",
    "        print(len(task_runs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "../evaluations/sentiment_beto.json\n",
      "../evaluations/sentiment_roberta_base.json\n",
      "../evaluations/sentiment_mbert_en.json\n",
      "../evaluations/sentiment_bert_base.json\n",
      "../evaluations/sentiment_distilbert_es.json\n",
      "../evaluations/sentiment_bertweet_base.json\n",
      "../evaluations/sentiment_mbert_es.json\n",
      "../evaluations/sentiment_distilbert_en.json\n",
      "../evaluations/emotion_bertweet_base.json\n",
      "../evaluations/emotion_beto.json\n",
      "../evaluations/emotion_mbert_es.json\n",
      "../evaluations/emotion_mbert_en.json\n",
      "../evaluations/emotion_roberta.json\n",
      "../evaluations/emotion_bert_base.json\n",
      "../evaluations/emotion_distilbert_en.json\n",
      "../evaluations/emotion_distilbert_es.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob \n",
    "import json \n",
    "\n",
    "tasks = [\"sentiment\", \"emotion\"]\n",
    "\n",
    "files = [(task, f) for task in tasks for f in glob.glob(f\"../evaluations/{task}_*.json\")]\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for task, file in files:\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        evaluation = json.load(f)\n",
    "        evaluation[\"task\"] = task\n",
    "        evaluation[\"file\"] = file.split(\"/\")[-1]\n",
    "        evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for task in tasks:\n",
    "    df =  pd.DataFrame([\n",
    "        {**evaluation, **evaluation[\"metrics\"]} for evaluation in evaluations if evaluation[\"task\"] == task\n",
    "    ])\n",
    "\n",
    "    df.drop(labels=[\"predictions\", \"labels\", \"metrics\", \"file\"], inplace=True, axis=1)\n",
    "    df[\"model\"] = df[\"model\"].str.replace(\"models/\", \"\")\n",
    "\n",
    "    df[\"model\"] = df[\"model\"].str.replace(f\"-{task}-analysis/\", \"\")\n",
    "    df.columns = [col.replace(\"test_\", \"\").replace(\"_\", \" \").capitalize() for col in df.columns]\n",
    "    #df.set_index(\"Model\", inplace=True)\n",
    "    df = df.sort_values([\"Lang\", \"Macro f1\"]) \n",
    "    dfs[task] = df\n",
    "\n",
    "df = dfs[\"sentiment\"].merge(dfs[\"emotion\"], on=\"Model\", suffixes=(\"\", \"_emotion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lang', 'Model', 'Task', 'Loss', 'Neg f1', 'Neg precision',\n",
       "       'Neg recall', 'Neu f1', 'Neu precision', 'Neu recall', 'Pos f1',\n",
       "       'Pos precision', 'Pos recall', 'Micro f1', 'Macro f1',\n",
       "       'Macro precision', 'Macro recall', 'Acc', 'Runtime',\n",
       "       'Samples per second', 'Lang_emotion', 'Task_emotion', 'Loss_emotion',\n",
       "       'Others f1', 'Others precision', 'Others recall', 'Joy f1',\n",
       "       'Joy precision', 'Joy recall', 'Sadness f1', 'Sadness precision',\n",
       "       'Sadness recall', 'Anger f1', 'Anger precision', 'Anger recall',\n",
       "       'Surprise f1', 'Surprise precision', 'Surprise recall', 'Disgust f1',\n",
       "       'Disgust precision', 'Disgust recall', 'Fear f1', 'Fear precision',\n",
       "       'Fear recall', 'Micro f1_emotion', 'Macro f1_emotion',\n",
       "       'Macro precision_emotion', 'Macro recall_emotion', 'Acc_emotion',\n",
       "       'Runtime_emotion', 'Samples per second_emotion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      "        Model &         Lang &     Micro f1 &     Macro f1 &  Micro f1\\_emotion &  Macro f1\\_emotion \\\\\n",
      "\\midrule\n",
      "distilbert-en &           en &        0.649 &        0.642 &             0.503 &             0.383 \\\\\n",
      "     mbert-en &           en &        0.645 &        0.643 &             0.516 &             0.394 \\\\\n",
      " roberta-base &           en &        0.686 &        0.684 &             0.563 &             0.445 \\\\\n",
      "    bert-base &           en &        0.686 &        0.684 &             0.559 &             0.439 \\\\\n",
      "bertweet-base &           en &        0.697 &        0.696 &             0.584 &             0.476 \\\\\n",
      "distilbert-es &           es &        0.602 &        0.599 &             0.600 &             0.463 \\\\\n",
      "     mbert-es &           es &        0.609 &        0.604 &             0.610 &             0.474 \\\\\n",
      "         beto &           es &        0.672 &        0.667 &             0.688 &             0.548 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_columns = [\"Model\", \"Lang\"]\n",
    "\n",
    "df.sort_values([\"Lang\", \"Macro f1\"], inplace=True) \n",
    "sentiment_columns = [\n",
    "    #\"Pos f1\", \n",
    "    #\"Neu f1\", \n",
    "    #\"Neg f1\", \n",
    "    \"Micro f1\",\n",
    "    \"Macro f1\"\n",
    "]\n",
    "emotion_columns = [\n",
    "    #\"Joy f1\",\n",
    "    #\"Others f1\",\n",
    "    #\"Sadness f1\",\n",
    "    #\"Anger f1\",\n",
    "    #\"Disgust f1\",\n",
    "    \"Micro f1_emotion\",\n",
    "    \"Macro f1_emotion\"\n",
    "]\n",
    "\n",
    "print(df[base_columns + sentiment_columns + emotion_columns].to_latex(index=False, float_format=\"{0:.3f}\".format, col_space=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         | Lang   |   Others f1 |   Joy f1 |   Sadness f1 |   Anger f1 |   Surprise f1 |   Disgust f1 |   Fear f1 |   Macro f1 |\n",
      "|:--------------|:-------|------------:|---------:|-------------:|-----------:|--------------:|-------------:|----------:|-----------:|\n",
      "| distilbert-en | en     |    0.509407 | 0.665484 |     0.502165 |   0.30303  |      0.147541 |     0.351111 |  0.201835 |   0.382939 |\n",
      "| mbert-en      | en     |    0.547718 | 0.650691 |     0.544643 |   0.29703  |      0.102941 |     0.356989 |  0.255319 |   0.393619 |\n",
      "| bert-base     | en     |    0.59398  | 0.683521 |     0.541485 |   0.355769 |      0.238411 |     0.408867 |  0.252632 |   0.439238 |\n",
      "| roberta-base  | en     |    0.573888 | 0.689527 |     0.547945 |   0.363636 |      0.212389 |     0.472656 |  0.255814 |   0.445122 |\n",
      "| bertweet-base | en     |    0.606019 | 0.711069 |     0.608696 |   0.433862 |      0.257669 |     0.452489 |  0.26     |   0.475686 |\n",
      "| distilbert-es | es     |    0.678962 | 0.60223  |     0.716707 |   0.434316 |      0.264368 |     0.084507 |  0.461538 |   0.463233 |\n",
      "| mbert-es      | es     |    0.691127 | 0.585242 |     0.710462 |   0.490566 |      0.273292 |     0.133333 |  0.431373 |   0.473628 |\n",
      "| beto          | es     |    0.74937  | 0.671916 |     0.753695 |   0.573684 |      0.396694 |     0.153846 |  0.533333 |   0.547506 |\n"
     ]
    }
   ],
   "source": [
    "f1_columns = [col for col in df.columns if \"f1\" in col and \"Macro\" not in col]\n",
    "print(df[[\"Model\", \"Lang\"] + f1_columns + [\"Macro f1\"]].to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pysent-oyXQVI9B': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b2ee3c7e4be117f16044e4287774c113d04cbc1cc9e7e3b16e6e098f73486a4"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
